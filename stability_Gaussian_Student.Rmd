---
title: "Stability: Gaussian vs Student-t"
author: "Jack Jewson"
date: "Dec 2022"
output: html_document
---

This notebook contains the code to reproduce Figures 1, 2, 3, B.1, B.2, B.3 of ''On the Stability of General Bayesian Inference'' Jewson, Smith and Holmes (2023) 

## Preliminaries {.tabset}


### Working directory

Setting the Working Directory as the folder where these files are stored

```{r wkd, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE}

my.dir <- "/home/usuario/Documents/Barcelona_Yr1/StabilityGeneralBayes"

```

### Packages

Loading required packages

```{r pcks, include = TRUE, echo = TRUE, eval = TRUE, cache = FALSE}

library(rstan)
rstan_options(auto_write = TRUE)

library(metRology)

library(numDeriv)
library(RColorBrewer)

```

### stan files

Compiling the required stan files

```{r stan, include = TRUE, echo = TRUE, eval = TRUE, cache = FALSE}

setwd(my.dir)

KLBayesnorm_sigma2_adj_stan <- stan_model(file = "stan/KLBayesnorm_var_sigma2_adj.stan")

KLBayest_stan <- stan_model(file = "stan/KLBayest_var.stan")

betaBayesnorm_sigma2_adj_stan <- stan_model(file = "stan/betaBayesnorm_var_sigma2_adj.stan")

betaBayest_stan <- stan_model(file = "stan/betaBayest_var.stan")

gammaBayesnorm_sigma2_adj_stan <- stan_model(file = "gammaBayesnorm_var_sigma2_adj.stan")

gammaBayest_stan <- stan_model(file = "gammaBayest_var.stan")



```


### Functions

Functions to calculate the calibration weight according to Lyddon, Holmes and Walker (2018)

```{r weight_calib, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE}


## Need to define the loss on an uncontrained paramater space
weight_calib <- function(data, loss, theta_0){
 p <- length(theta_0)
 n <- length(data)
 #theta_hat <- optim(theta_0, function(theta){loss(data, theta)}, gr = function(theta){grad(function(theta){loss(data, theta)}, theta)}, method = "BFGS")
 theta_hat <- optim(theta_0, function(theta){loss(data, theta)})

 grad_data <- matrix(NA, nrow = n, ncol = p)
 Hess_data <- array(NA, dim = c(n, p, p))
 mean_grad2_data <- matrix(0, nrow = p, ncol = p)
 mean_Hess_data <- matrix(0, nrow = p, ncol = p)
 for(i in 1:n){
 grad_data[i, ] <- grad(function(theta){loss(data[i], theta)}, theta_hat$par)
 mean_grad2_data <- mean_grad2_data+grad_data[i, ]%*%t(grad_data[i, ])
 Hess_data[i, , ] <- hessian(function(theta){loss(data[i], theta)}, theta_hat$par)
 mean_Hess_data <- mean_Hess_data+Hess_data[i, , ]
 #if(i%%(n/20) == 1){cat("Observation", i, "done", "\n")}
 }
 hat_I_theta_data <- mean_grad2_data/n
 hat_J_theta_data <- mean_Hess_data/n

 w_data <- sum(diag((hat_J_theta_data%*%solve(hat_I_theta_data)%*%t(hat_J_theta_data))))/sum(diag(hat_J_theta_data))

 return(w_data)
}

KLD_loss_norm <- function(y, mu, sigma2){
 return(-sum(dnorm(y, mu, sqrt(sigma2), log = TRUE)))
}


KLD_loss_t <- function(y, mu, sigma2, df){
 return(-sum(dt.scaled(y, df, mu, sqrt(sigma2), log = TRUE)))
}


beta_loss_norm <- function(y, mu, sigma2, beta){
 integral_term <- 1/((2*pi)^(beta/2)*(1+beta)^1.5*((sigma2)^(beta/2)))
 likelihood_term <- (1/beta)*dnorm(y, mu, sqrt(sigma2))^(beta)
 return(-sum(likelihood_term-integral_term))
}

beta_loss_norm_sigma2_adj <- function(y, mu, sigma2, sigma2_adj, beta){
 integral_term <- 1/((2*pi)^(beta/2)*(1+beta)^1.5*((sigma2*sigma2_adj)^(beta/2)))
 likelihood_term <- (1/beta)*dnorm(y, mu, sqrt(sigma2*sigma2_adj))^(beta)
 return(-sum(likelihood_term-integral_term))
}

beta_loss_t <- function(y, mu, sigma2, df, beta){
 integral_term <- (gamma((df+1)/2)^(beta+1)*gamma((beta*df+beta+df)/2))/((1+beta)*gamma(df/2)^(beta+1)*gamma((beta*df+beta+df+1)/2)*(df)^((beta)/2)*pi^((beta)/2)*sigma2^(beta/2))
 likelihood_term <- (1/beta)*dt.scaled(y, df, mu, sqrt(sigma2))^(beta)
 return(-sum(likelihood_term-integral_term))
}

gamma_loss_norm_sigma2_adj <- function(y, mu, sigma2, sigma2_adj, gamma){
 integral_term <- 1/((2*pi)^(gamma/2)*(1+gamma)^0.5*((sigma2*sigma2_adj)^(gamma/2)))
 likelihood_term <- (1/gamma)*dnorm(y, mu, sqrt(sigma2*sigma2_adj))^(gamma)*1/(gamma + 1)*1/(integral_term^(gamma/(gamma + 1)))
 return(-sum(likelihood_term-integral_term))
}

gamma_loss_t <- function(y, mu, sigma2, df, gamma){
 integral_term <- (gamma((df+1)/2)^(gamma+1)*gamma((gamma*df+gamma+df)/2))/(gamma(df/2)^(gamma+1)*gamma((gamma*df+gamma+df+1)/2)*(df)^((gamma)/2)*pi^((gamma)/2)*sigma2^(gamma/2))
 likelihood_term <- (1/gamma)*dt.scaled(y, df, mu, sqrt(sigma2))^(gamma)
 return(-sum(likelihood_term-integral_term))
}

```

## Gaussian and Student-t TVD Neighbourhood 

Constructing the TVD neighbourhood including the Gaussian and the Student's-t likelihood

```{r t_normal_neighbourhood, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE, dev = tikzDevice::tikz()}


## estimating sigma^2_adj to match the quartiles
sigma2_adj_fun <- function(sigma2_adj){
 return(abs(qt.scaled(0.25, df = 5, 0, 1)-qnorm(0.25, 0, sqrt(sigma2_adj*1))))
}

sigma2_adj <- optimize(sigma2_adj_fun, 1, lower = 0, upper = 1000)
sigma2_adj$minimum

## Note this holds for all mu and sigma2

df <- 5
mu <- 0
sigma2 <- 2

qt.scaled(0.25, df = df, mu, sqrt(sigma2)) - qnorm(0.25, mu, sqrt(sigma2_adj$minimum*sigma2))

df <- 5
mu <- 1
sigma2 <- 4

qt.scaled(0.25, df = df, mu, sqrt(sigma2)) - qnorm(0.25, mu, sqrt(sigma2_adj$minimum*sigma2))

df <- 5
mu <- 0
sigma2 <- 1

qt.scaled(0.25, df = df, mu, sqrt(sigma2)) - qnorm(0.25, mu, sqrt(sigma2_adj$minimum*sigma2))

## estimating the TVD between the two likelihoods 

norm_t_diff <- function(x){abs(dnorm(x, mu, sqrt(sigma2*out_optim$minimum))-dt.scaled(x, df, mu, sqrt(sigma2)))}

inter1 <- optimize(norm_t_diff, c(-2, -1))
inter2 <- optimize(norm_t_diff, c(-1, 0))
inter3 <- optimize(norm_t_diff, c(0, 1))
inter4 <- optimize(norm_t_diff, c(1, 2))

inter1$minimum
inter2$minimum
inter3$minimum
inter4$minimum


TVD_est <- 2*((pnorm(inter2$minimum, mu, sqrt(sigma2*out_optim$minimum))-pnorm(inter1$minimum, mu, sqrt(sigma2*out_optim$minimum)))-
(pt.scaled(inter2$minimum, df, mu, sqrt(sigma2))-pt.scaled(inter1$minimum, df, mu, sqrt(sigma2))))

TVD_est

```

The pdfs and cdf of the two models

```{r t_normal_neighbourhood, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE, fig.height = 3, fig.width = 5, dev = tikzDevice::tikz()}

par(mar = c(3.1, 3.3, 1.5, 1.1)) # bottom, left, top, right
#par(mar = c(5.1, 4.1, 4.1, 2.1)) # Default
#par(mgp = c(3, 1, 0)) # Default - location of xlab and ylab, tick-mark labels, tick marks.
par(mgp = c(2.15, 1, 0))
par(cex.lab = 1.25, cex.axis = 1.25, cex.main = 1.25)

#par(mar = c(5.1, 4.5, 4.1, 2.1), mgp = c(3, 1, 0), las = 0)
x <- seq(-5, 5, length.out = 1000)
plot(x, dnorm(x, mu, sqrt(sigma2*sigma2_adj$minimum)), type = "l", lwd = 3, col = "red", xlab = "$y$", ylab = "Density")
points(x, dt.scaled(x, df, mu, sqrt(sigma2)), type = "l", lwd = 3, col = "blue")
legend(-5, 0.35, c("Gaussian", "Student's-$t$"), lty = c(1, 1), lwd = c(3, 3), col = c("red", "blue"), bty = "n", cex = 1.25)
box(which = "plot")

#par(mar = c(5.1, 4.5, 4.1, 2.1), mgp = c(3, 1, 0), las = 0)
plot(x, pnorm(x, mu, sqrt(sigma2*sigma2_adj$minimum)), type = "l", lwd = 3, col = "red", xlab = "$y$", ylab = "Cumulative Density")
points(x, pt.scaled(x, df, mu, sqrt(sigma2)), type = "l", lwd = 3, col = "blue")


```



## epsilon-contamination (KLD vs betaD) {.tabset}

Generating 1000 observations from $g(y) = 0.9\times\mathcal{N}\left(y;0,1\right) + 0.1 \times \mathcal{N}\left(y;5,3^2\right)$

### Data generation


```{r eps_cont_data_sim, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE}

n <- 1000
set.seed(3)
mu_c <- 5
sig_c <- 3
mu <- 0
sig <- 1
eps <- 0.1

cont <- sample(c(0, 1), n, replace = TRUE, prob = c(1-eps, eps))
data_eps_cont <- (1-cont)*rnorm(n, mu, sig)+cont*rnorm(n, mu_c, sig_c)

cont_ind <- which(cont == 1)

```


### Prior Hyperparameters 

```{r prior_hyperparameters, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE}

mu_0 <- 0
kappa_0 <- 1/10
a_0 <- 0.01
b_0 <- 0.01

```

### Gaussian Model - KLD-Bayes

```{r eps_cont_KL_norm, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE}

N <- 100000


KLBayesnorm_data <- list(n = n, y = matrix(data_eps_cont, nrow = n, ncol = 1), mu_m = mu_0, mu_s = 1/kappa_0, sig_p1 = a_0, sig_p2 = b_0, w = 1, sigma2_adj = sigma2_adj$minimum)
KLBayesnorm <- sampling(object = KLBayesnorm_sigma2_adj_stan, data = KLBayesnorm_data, iter = N+5000, warmup = 5000, chains = 1, cores = 1, control = list(adapt_delta = 0.999, stepsize = 0.01))
KLBayesnorm_params <- extract(KLBayesnorm)



```

### Student-t Model - KLD-Bayes

```{r eps_cont_KL_t, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE}

N <- 100000

KLBayest_data <- list(n = n, y = matrix(data_eps_cont, nrow = n, ncol = 1), mu_m = mu_0, mu_s = 1/kappa_0, sig_p1 = a_0, sig_p2 = b_0, df = 5, w = 1)
KLBayest <- sampling(object = KLBayest_stan, data = KLBayest_data, iter = N+5000, warmup = 5000, chains = 1, cores = 1, control = list(adapt_delta = 0.999, stepsize = 0.01))
KLBayest_params <- extract(KLBayest)


```


### Comparitive Plots

Comparing the predictive and parameter posterior distributions 

```{r eps_cont_KL_norm_t, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE, dev = tikzDevice::tikz()}

mean(KLBayesnorm_params$sigma2)*out_optim$minimum
mean(KLBayest_params$sigma2)*5/(5-2)

par(mar = c(5.1, 4.5, 4.1, 2.1), mgp = c(3, 1, 0), las = 0)

x <- seq(-5, 15, length.out = 1000)
hist_data1 <- hist(data_eps_cont[-cont_ind], breaks = seq(-5, 17, by = 0.2), plot = FALSE)## need to factor in the length of the vector and the number of breaks.
hist_data2 <- hist(data_eps_cont[cont_ind], breaks = seq(-5, 17, by = 0.2), plot = FALSE)
plot(0, 0, type = "n", ylab = "Density", main = expression(paste("KLD")), xlab = "$y$", ylim = c(0, 0.45), xlim = c(-5, 12), cex.lab = 2, cex.axis = 2, cex.main = 2)
#hist_data1$counts <- hist_data1$counts/(n/2)# this guy is made to be equal to 5 as the lengths of each of our bars is 0.2
hist_data1$counts <- hist_data1$counts/(n/5)
hist_data2$counts <- hist_data2$counts/(n/5)
plot(hist_data1, add = TRUE, col = "grey")
plot(hist_data2, add = TRUE, col = "black")
x <- seq(-10, 15, length.out = 1000)
lines(density(KLBayesnorm_params$y_predict), lwd = 3, col = "red")
lines(density(KLBayest_params$y_predict), lwd = 3, col = "blue")
legend(3, 0.45, c(expression(paste((1-epsilon), "N(0, 1)")), expression(paste(epsilon, "N(5, ", 3^2, ")")), "Gaussian", "Student's-t"), lty = c(1, 1, 1, 1), lwd = c(3, 3, 3, 3), col = c("grey", "black", "red", "blue"), bty = "n", cex = 1.5)
box(which = "plot")

plot(density(KLBayesnorm_params$mu), lwd = 3, col = "red", xlim = c(-0.25, 0.75), ylim = c(0, 10), xlab = expression(mu), cex.lab = 2, cex.axis = 2, main = expression(paste("KLD")), cex.main = 2)
lines(density(KLBayest_params$mu), lwd = 3, col = "blue")

plot(density(KLBayesnorm_params$sigma2), lwd = 3, col = "red", xlim = c(0.5, 4.5), ylim = c(0, 5.5), xlab = expression(sigma^2), cex.lab = 2, cex.axis = 2, main = expression(paste("KLD")), cex.main = 2)
lines(density(KLBayest_params$sigma2), lwd = 3, col = "blue")


#plot(density(KLBayesnorm_params$sigma2*sigma2_adj$minimum), lwd = 3, col = "red", xlim = c(1.5, 5), ylim = c(0, 3.5), xlab = "Model Variance", main = "")
#lines(density(KLBayest_params$sigma2*5/(5-2)), lwd = 3, col = "blue")

## while it will be hard to estimate the TVD between the two predictives here, we may be able to estimate something like the energy distance which could serve as a surrogate, 

```

Calculating the energy distance $D_E(g, f): = \int (G(x)-F(x))^2dx$ which is conveniently equivalent to $2\mathbb{E}_{X\sim G}\mathbb{E}_{Y\sim F}\left|X-Y\right|-\mathbb{E}_{X\sim G}\mathbb{E}_{X^{'}\sim G}\left|X-X^{'}\right|-\mathbb{E}_{Y\sim F}\mathbb{E}_{Y\sim F^{'}}\left|Y-Y^{'}\right|$


```{r eps_cont_KL_norm_t_E_distance, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE}
energy_distance <- function(f1_samp, f2_samp){
 N <- length(f1_samp)
 E_distance <- 0
 for(i in 1:N){
 E_distance <- E_distance + 2/N^2*sum(abs(f1_samp[i]-f2_samp)) - 1/(N*(N-1))*sum(abs(f2_samp[i]-f2_samp)) - 1/(N*(N-1))*sum(abs(f1_samp[i]-f1_samp))
 }
 return(E_distance)
}

energy_distance(KLBayesnorm_params$y_predict, KLBayest_params$y_predict)


```

### Gaussian Model - betaD-Bayes


```{r eps_cont_beta_norm, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE}
beta <- 0.22 #- Learned using Yonekura & Sugasawa

N <- 100000

betaD_w_norm <- weight_calib(data_eps_cont, loss = function(data, theta){beta_loss_norm_sigma2_adj(data, theta[1], theta[2], sigma2_adj = sigma2_adj$minimum, beta)}, theta_0 = c(mean(data_eps_cont), var(data_eps_cont)))


betaBayesnorm_data <- list(n = n, y = matrix(data_eps_cont, nrow = n, ncol = 1), mu_m = mu_0, mu_s = 1/kappa_0, sig_p1 = a_0, sig_p2 = b_0, w = betaD_w_norm, sigma2_adj = sigma2_adj$minimum, beta = beta)
betaBayesnorm <- sampling(object = betaBayesnorm_sigma2_adj_stan, data = betaBayesnorm_data, iter = N+5000, warmup = 5000, chains = 1, cores = 1, control = list(adapt_delta = 0.95))#, stepsize = 0.01))
betaBayesnorm_params <- extract(betaBayesnorm)



```

### Student-t Model - betaD-Bayes

```{r eps_cont_beta_t, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE}
beta <- 0.22 #- Learned using Yonekura & Sugasawa

N <- 100000


betaD_w_t <- weight_calib(data_eps_cont, loss = function(data, theta){beta_loss_t(data, theta[1], theta[2], df = 5, beta)}, theta_0 = c(mean(data_eps_cont), var(data_eps_cont)))
betaBayest_data <- list(n = n, y = matrix(data_eps_cont, nrow = n, ncol = 1), mu_m = mu_0, mu_s = 1/kappa_0, sig_p1 = a_0, sig_p2 = b_0, df = 5, w = betaD_w_t, beta = beta)
betaBayest <- sampling(object = betaBayest_stan, data = betaBayest_data, iter = N+5000, warmup = 5000, chains = 1, cores = 1, control = list(adapt_delta = 0.95))#, stepsize = 0.01))
betaBayest_params <- extract(betaBayest)




```

### Comparitive Plots

Comparing the predictive and parameter posterior distributions


```{r eps_cont_beta_norm_t, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE, dev = tikzDevice::tikz()}

mean(betaBayesnorm_params$sigma2)*out_optim$minimum
mean(betaBayest_params$sigma2)*5/(5-2)

par(mar = c(5.1, 4.5, 4.1, 2.1), mgp = c(3, 1, 0), las = 0)

x <- seq(-5, 15, length.out = 1000)
hist_data1 <- hist(data_eps_cont[-cont_ind], breaks = seq(-5, 17, by = 0.2), plot = FALSE)## need to factor in the length of the vector and the number of breaks.
hist_data2 <- hist(data_eps_cont[cont_ind], breaks = seq(-5, 17, by = 0.2), plot = FALSE)
plot(0, 0, type = "n", ylab = "Density", main = expression(paste(beta, "D")), xlab = "$y$", ylim = c(0, 0.45), xlim = c(-5, 12), cex.lab = 2, cex.axis = 2, cex.main = 2)
#hist_data1$counts <- hist_data1$counts/(n/2)# this guy is made to be equal to 5 as the lengths of each of our bars is 0.2
hist_data1$counts <- hist_data1$counts/(n/5)
hist_data2$counts <- hist_data2$counts/(n/5)
plot(hist_data1, add = TRUE, col = "grey")
plot(hist_data2, add = TRUE, col = "black")
x <- seq(-10, 15, length.out = 1000)
lines(density(betaBayesnorm_params$y_predict), lwd = 3, col = "red")
lines(density(betaBayest_params$y_predict), lwd = 3, col = "blue")
legend(3, 0.45, c(expression(paste((1-epsilon), "N(0, 1)")), expression(paste(epsilon, "N(5, ", 3^2, ")")), "Gaussian", "Student's-t"), lty = c(1, 1, 1, 1), lwd = rep(3, 4), col = c("grey", "black", "red", "blue"), bty = "n", cex = 1.5)
box(which = "plot")

plot(density(KLBayesnorm_params$mu), lwd = 3, col = "white", xlim = c(-0.25, 0.25), ylim = c(0, 10.75), xlab = expression(mu), main = expression(paste(beta, "D")), cex.lab = 2, cex.axis = 2, cex.main = 2)
lines(density(betaBayesnorm_params$mu), lwd = 3, col = "red", xlim = c(-0.25, 0.25), ylim = c(0, 11), xlab = expression(mu), main = expression(paste(beta, "D")), cex.lab = 2, cex.axis = 2)
lines(density(betaBayest_params$mu), lwd = 3, col = "blue")

plot(density(betaBayesnorm_params$sigma2), lwd = 3, col = "red", xlim = c(0.5, 1.5), ylim = c(0, 8), xlab = expression(sigma^2), main = expression(paste(beta, "D")), cex.lab = 2, cex.axis = 2, cex.main = 2)
lines(density(betaBayest_params$sigma2), lwd = 3, col = "blue")

#plot(density(betaBayesnorm_params$sigma2*out_optim$minimum), lwd = 3, col = "red", xlim = c(0.5, 2.5), ylim = c(0, 3.5), xlab = expression(sigma^2), main = "")
#lines(density(betaBayest_params$sigma2), lwd = 3, col = "blue")

#plot(density(betaBayesnorm_params$sigma2*sigma2_adj$minimum), lwd = 3, col = "red", xlim = c(0.5, 2.5), ylim = c(0, 3.5), xlab = "Model Variance", main = "")
#lines(density(betaBayest_params$sigma2*5/(5-2)), lwd = 3, col = "blue")

## while it will be hard to estimate the TVD between the two predictives here, we may be able to estimate something like the energy distance which could serve as a surrogate, 



```


```{r eps_cont_beta_norm_t_E_distance, include = TRUE, echo = TRUE, eval = FALSE, cache = TRUE}

energy_distance(betaBayesnorm_params$y_predict, betaBayest_params$y_predict)

```


### Gaussian Model - gammaD-Bayes

```{r eps_cont_gamma_norm, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE}
gamma <- 0.22 #- Learned using Yonekura & Sugasawa for the betaD

N <- 100000

gammaD_w_norm <- weight_calib(data_eps_cont, loss = function(data, theta){gamma_loss_norm_sigma2_adj(data, theta[1], theta[2], sigma2_adj = sigma2_adj$minimum, gamma)}, theta_0 = c(mean(data_eps_cont), var(data_eps_cont)))


gammaBayesnorm_data <- list(n = n, y = matrix(data_eps_cont, nrow = n, ncol = 1), mu_m = mu_0, mu_s = 1/kappa_0, sig_p1 = a_0, sig_p2 = b_0, w = gammaD_w_norm, sigma2_adj = sigma2_adj$minimum, gamma = gamma)
gammaBayesnorm <- sampling(object = gammaBayesnorm_sigma2_adj_stan, data = gammaBayesnorm_data, iter = N+5000, warmup = 5000, chains = 1, cores = 1, control = list(adapt_delta = 0.95))#, stepsize = 0.01))
gammaBayesnorm_params <- extract(gammaBayesnorm)



```

### Student-t Model - gammaD-Bayes

```{r eps_cont_gamma_t, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE}
gamma <- 0.22 #- Learned using Yonekura & Sugasawa for the betaD

N <- 100000


gammaD_w_t <- weight_calib(data_eps_cont, loss = function(data, theta){gamma_loss_t(data, theta[1], theta[2], df = 5, gamma)}, theta_0 = c(mean(data_eps_cont), var(data_eps_cont)))
gammaBayest_data <- list(n = n, y = matrix(data_eps_cont, nrow = n, ncol = 1), mu_m = mu_0, mu_s = 1/kappa_0, sig_p1 = a_0, sig_p2 = b_0, df = 5, w = gammaD_w_t, gamma = gamma)
gammaBayest <- sampling(object = gammaBayest_stan, data = gammaBayest_data, iter = N+5000, warmup = 5000, chains = 1, cores = 1, control = list(adapt_delta = 0.95))#, stepsize = 0.01))
gammaBayest_params <- extract(gammaBayest)




```

### Comparitive Plots

Comparing the predictive and parameter posterior distributions


```{r eps_cont_gamma_norm_t, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE, dev = tikzDevice::tikz()}

mean(gammaBayesnorm_params$sigma2)*out_optim$minimum
mean(gammaBayest_params$sigma2)*5/(5-2)

par(mar = c(5.1, 4.5, 4.1, 2.1), mgp = c(3, 1, 0), las = 0)

x <- seq(-5, 15, length.out = 1000)
hist_data1 <- hist(data_eps_cont[-cont_ind], breaks = seq(-5, 17, by = 0.2), plot = FALSE)## need to factor in the length of the vector and the number of breaks.
hist_data2 <- hist(data_eps_cont[cont_ind], breaks = seq(-5, 17, by = 0.2), plot = FALSE)
plot(0, 0, type = "n", ylab = "Density", main = expression(paste(gamma, "D")), xlab = "$y$", ylim = c(0, 0.45), xlim = c(-5, 12), cex.lab = 2, cex.axis = 2, cex.main = 2)
#hist_data1$counts <- hist_data1$counts/(n/2)# this guy is made to be equal to 5 as the lengths of each of our bars is 0.2
hist_data1$counts <- hist_data1$counts/(n/5)
hist_data2$counts <- hist_data2$counts/(n/5)
plot(hist_data1, add = TRUE, col = "grey")
plot(hist_data2, add = TRUE, col = "black")
x <- seq(-10, 15, length.out = 1000)
lines(density(gammaBayesnorm_params$y_predict), lwd = 3, col = "red")
lines(density(gammaBayest_params$y_predict), lwd = 3, col = "blue")
legend(3, 0.45, c(expression(paste((1-epsilon), "N(0, 1)")), expression(paste(epsilon, "N(5, ", 3^2, ")")), "Gaussian", "Student's-t"), lty = c(1, 1, 1, 1), lwd = rep(3, 4), col = c("grey", "black", "red", "blue"), bty = "n", cex = 1.5)
box(which = "plot")

plot(density(KLBayesnorm_params$mu), lwd = 3, col = "white", xlim = c(-0.25, 0.25), ylim = c(0, 10.75), xlab = expression(mu), main = expression(paste(gamma, "D")), cex.lab = 2, cex.axis = 2, cex.main = 2)
lines(density(gammaBayesnorm_params$mu), lwd = 3, col = "red", xlim = c(-0.25, 0.25), ylim = c(0, 11), xlab = expression(mu), main = expression(paste(gamma, "D")), cex.lab = 2, cex.axis = 2)
lines(density(gammaBayest_params$mu), lwd = 3, col = "blue")

plot(density(gammaBayesnorm_params$sigma2), lwd = 3, col = "red", xlim = c(0.5, 1.5), ylim = c(0, 8), xlab = expression(sigma^2), main = expression(paste(gamma, "D")), cex.lab = 2, cex.axis = 2, cex.main = 2)
lines(density(gammaBayest_params$sigma2), lwd = 3, col = "blue")

#plot(density(gammaBayesnorm_params$sigma2*out_optim$minimum), lwd = 3, col = "red", xlim = c(0.5, 2.5), ylim = c(0, 3.5), xlab = expression(sigma^2), main = "")
#lines(density(gammaBayest_params$sigma2), lwd = 3, col = "blue")

#plot(density(gammaBayesnorm_params$sigma2*sigma2_adj$minimum), lwd = 3, col = "red", xlim = c(0.5, 2.5), ylim = c(0, 3.5), xlab = "Model Variance", main = "")
#lines(density(gammaBayest_params$sigma2*5/(5-2)), lwd = 3, col = "blue")

## while it will be hard to estimate the TVD between the two predictives here, we may be able to estimate something like the energy distance which could serve as a surrogate, 



```


```{r eps_cont_gamma_norm_t_E_distance, include = TRUE, echo = TRUE, eval = FALSE, cache = TRUE}



energy_distance(gammaBayesnorm_params$y_predict, gammaBayest_params$y_predict)

## I suppose we could maybe also estimate the gammaD

```


## Sensitivity analysis

Running the above betaD-Bayes inference for a few values of $\beta$. 

```{r Gaussian_t_sensitivity, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE}
beta_vec <- c(0, 0.001, 0.01, seq(0.05, 1, by = 0.05))

N <- 10000


B <- length(beta_vec)
E_distance_beta <- rep(NA, B)

Gaussian_y_predict <- matrix(NA, nrow = length(beta_vec), ncol = N)
Student_y_predict <- matrix(NA, nrow = length(beta_vec), ncol = N)

for(j in 1:B){
 if(beta_vec[j] == 0){
 ## Normal likelihood
 KLBayesnorm_data <- list(n = n, y = matrix(data_eps_cont, nrow = n, ncol = 1), mu_m = mu_0, mu_s = 1/kappa_0, sig_p1 = a_0, sig_p2 = b_0, w = 1, sigma2_adj = sigma2_adj$minimum)
 KLBayesnorm <- sampling(object = KLBayesnorm_sigma2_adj_stan, data = KLBayesnorm_data, iter = N+5000, warmup = 5000, chains = 1, cores = 1, control = list(adapt_delta = 0.999, stepsize = 0.01), refresh = 0)
 KLBayesnorm_params <- extract(KLBayesnorm)
 ## Student's-t likelihood 
 KLBayest_data <- list(n = n, y = matrix(data_eps_cont, nrow = n, ncol = 1), mu_m = mu_0, mu_s = 1/kappa_0, sig_p1 = a_0, sig_p2 = b_0, df = 5, w = 1)
 KLBayest <- sampling(object = KLBayest_stan, data = KLBayest_data, iter = N+5000, warmup = 5000, chains = 1, cores = 1, control = list(adapt_delta = 0.999, stepsize = 0.01), refresh = 0)
 KLBayest_params <- extract(KLBayest)
 ## The Energy distance
 E_distance_beta[j] <- energy_distance(KLBayesnorm_params$y_predict, KLBayest_params$y_predict)
 Gaussian_y_predict[j,] <- KLBayesnorm_params$y_predict
 Student_y_predict[j,] <- KLBayest_params$y_predict
 } else{
 ## Normal likelihood
 betaD_w_norm <- weight_calib(data_eps_cont, loss = function(data, theta){beta_loss_norm_sigma2_adj(data, theta[1], theta[2], sigma2_adj = sigma2_adj$minimum, beta_vec[j])}, theta_0 = c(mean(data_eps_cont), var(data_eps_cont)))
 betaBayesnorm_data <- list(n = n, y = matrix(data_eps_cont, nrow = n, ncol = 1), mu_m = mu_0, mu_s = 1/kappa_0, sig_p1 = a_0, sig_p2 = b_0, w = betaD_w_norm, sigma2_adj = sigma2_adj$minimum, beta = beta_vec[j])
 betaBayesnorm <- sampling(object = betaBayesnorm_sigma2_adj_stan, data = betaBayesnorm_data, iter = N+5000, warmup = 5000, chains = 1, cores = 1, control = list(adapt_delta = 0.95))#, stepsize = 0.01))
 betaBayesnorm_params <- extract(betaBayesnorm)
 ## Student's-t likelihood 
 betaD_w_t <- weight_calib(data_eps_cont, loss = function(data, theta){beta_loss_t(data, theta[1], theta[2], df = 5, beta_vec[j])}, theta_0 = c(mean(data_eps_cont), var(data_eps_cont)))
 betaBayest_data <- list(n = n, y = matrix(data_eps_cont, nrow = n, ncol = 1), mu_m = mu_0, mu_s = 1/kappa_0, sig_p1 = a_0, sig_p2 = b_0, df = 5, w = betaD_w_t, beta = beta_vec[j])
 betaBayest <- sampling(object = betaBayest_stan, data = betaBayest_data, iter = N+5000, warmup = 5000, chains = 1, cores = 1, control = list(adapt_delta = 0.95))#, stepsize = 0.01))
betaBayest_params <- extract(betaBayest)
 ## The Energy distance
 E_distance_beta[j] <- energy_distance(betaBayesnorm_params$y_predict, betaBayest_params$y_predict)
  Gaussian_y_predict[j,] <- betaBayesnorm_params$y_predict
  Student_y_predict[j,] <- betaBayest_params$y_predict
 }

 
 
 
}


```

Plotting the posterior predictive of the Gaussian and the Student-t for different values of beta. Also plotting the energ distance between the two posteriors and 'neighbourhood multipler' from Theorems 1 and 2.

```{r Gaussian_t_sensitivity_plot_tikz, include = TRUE, echo = TRUE, eval = TRUE, cache = FALSE, fig.height = 3, fig.width = 5, dev = tikzDevice::tikz()}

par(mar = c(3.1, 3.3, 1.5, 1.1)) # bottom, left, top, right
#par(mar = c(5.1, 4.1, 4.1, 2.1)) # Default
#par(mgp = c(3, 1, 0)) # Default - location of xlab and ylab, tick-mark labels, tick marks.
par(mgp = c(2.15, 1, 0))
par(cex.lab = 1.25, cex.axis = 1.25, cex.main = 1.25)

M <- c(0.1, 0.25, 0.5, 1, 2, 5)
light_offset <- 3
cols <- rev(brewer.pal(n = length(M) + light_offset, name = "PuRd"))

plot(1 + beta_vec, M[1]^(beta_vec)*(2*beta_vec + 1)/(beta_vec*(beta_vec + 1)), xlim = c(1, 2), ylim = c(0, 20), xlab = "$\\beta$", ylab = "Neighbourhood Multiplier", type = "l", lwd = 3, col = cols[1])
for(m in 2:length(M)){
  #points(1 + beta_vec, M[m]^(beta_vec)/(beta_vec), type = "l", lwd = 3, col = cols[m])
  points(1 + beta_vec, M[m]^(beta_vec)*(2*beta_vec + 1)/(beta_vec*(beta_vec + 1)), type = "l", lwd = 3, col = cols[m])
}
legend("topright", c("$M = 5$", "$M = 2$", "$M = 1$", "$M = 0.5$", "$M = 0.25$", "$M = 0.1$"), lwd = 3, col = rev(cols)[(light_offset+1):(length(M) + light_offset)], bty = "n", cex = 1.25)

plot(1 + beta_vec, E_distance_beta, xlim = c(1, 2), ylim = c(0, 0.12), xlab = "$\\beta$", ylab = "Energy Distance", type = "b", lwd = 3, col = "black")


par(mar = c(3.7, 3.8, 1.5, 1.1)) # bottom, left, top, right
#par(mar = c(5.1, 4.1, 4.1, 2.1)) # Default
#par(mgp = c(3, 1, 0)) # Default - location of xlab and ylab, tick-mark labels, tick marks.
par(mgp = c(2.5, 1, 0))
par(cex.lab = 1.5, cex.axis = 1.5, cex.main = 1.5)

for(j in 1:B){
  x <- seq(-5, 15, length.out = 1000)
  hist_data1 <- hist(data_eps_cont[-cont_ind], breaks = seq(-5, 17, by = 0.2), plot = FALSE)## need to factor in the length of the vector and the number of breaks.
  hist_data2 <- hist(data_eps_cont[cont_ind], breaks = seq(-5, 17, by = 0.2), plot = FALSE)
  plot(0, 0, type = "n", ylab = "Density", main = paste("$\\beta = $",beta_vec[j] + 1), xlab = "$y$", ylim = c(0, 0.45), xlim = c(-5, 12))
  hist_data1$counts <- hist_data1$counts/(n/5)
  hist_data2$counts <- hist_data2$counts/(n/5)
  plot(hist_data1, add = TRUE, col = "grey")
  plot(hist_data2, add = TRUE, col = "black")
  x <- seq(-10, 15, length.out = 1000)
  lines(density(Gaussian_y_predict[j,]), lwd = 4, col = "red")
  lines(density(Student_y_predict[j,]), lwd = 4, col = "blue")
  if(j == 5){
    legend(3, 0.45, c("$(1-\\epsilon)\\mathcal{N}(0, 1)$", "$\\epsilon\\mathcal{N}(5, 3^2)$", "Gaussian", "Student's-$t$"), lty = c(1, 1, 1, 1), lwd = c(4, 4, 4, 4), col = c("grey", "black", "red", "blue"), bty = "n", cex = 1.5)
  }
  box(which = "plot")
}


```

## A uncotaminated case

These are to demonstrate the results of Section 4

```{r epsilon_contamination, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE, dev = tikzDevice::tikz()}

mu_c <- 5
sig_c <- 3
mu <- 0
sig <- 1
eps <- 0.1

dg1 <- function(x){
  return(dnorm(x, mu, sig))
}

dg2 <- function(x){
  return((1-eps)*dnorm(x, mu, sig) + eps*dnorm(x, mu_c, sig_c))
}
  

```

```{r epsilon_contamination_tikz, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE, fig.height = 3, fig.width = 5, dev = "tikz"}

par(mar = c(3.1, 3.3, 1.5, 1.1)) # bottom, left, top, right
#par(mar = c(5.1, 4.1, 4.1, 2.1)) # Default
#par(mgp = c(3, 1, 0)) # Default - location of xlab and ylab, tick-mark labels, tick marks.
par(mgp = c(2.15, 1, 0))
par(cex.lab = 1.25, cex.axis = 1.25, cex.main = 1.25)

x_seq <- seq(-5, 10, length.out = 1000)
plot(x_seq, abs(dg1(x_seq) - dg2(x_seq)), type = "l", xlab = "$y$", ylab = "$|g_1(y) - g_2(y)|$", lwd = 3)

x_seq <- seq(-5, 10, length.out = 1000)
plot(x_seq, abs((dg1(x_seq) - dg2(x_seq))*dnorm(x_seq, 0, 1, log = TRUE)), type = "l", xlab = "$y$", ylab = "$|(g_1(y) - g_2(y))\\log f(y)|$", lwd = 3)


```




```{r norm_data_sim, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE}
n <- 1000
set.seed(3)
mu <- 0
sig <- 1

data_norm <- rnorm(n, mu, sig)


mu_0 <- 0
kappa_0 <- 1/10
a_0 <- 0.01
b_0 <- 0.01




```


```{r norm_KL_norm, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE}

N <- 100000


KLBayesnorm_data <- list(n = n, y = matrix(data_norm, nrow = n, ncol = 1), mu_m = mu_0, mu_s = 1/kappa_0, sig_p1 = a_0, sig_p2 = b_0, w = 1, sigma2_adj = sigma2_adj$minimum)
KLBayesnorm <- sampling(object = KLBayesnorm_sigma2_adj_stan, data = KLBayesnorm_data, iter = N+5000, warmup = 5000, chains = 1, cores = 1, control = list(adapt_delta = 0.999, stepsize = 0.01))
KLBayesnorm_params <- extract(KLBayesnorm)



```


Plot predictives, parameter posterior, and maybe also the posterior for the variance of the data, this is quite tangible 

Remember the energy distance is $D_E(g, f): = \int (G(x)-F(x))^2dx$ which is conveniently equivalent to $2\mathbb{E}_{X\sim G}\mathbb{E}_{Y\sim F}\left|X-Y\right|-\mathbb{E}_{X\sim G}\mathbb{E}_{X^{'}\sim G}\left|X-X^{'}\right|-\mathbb{E}_{Y\sim F}\mathbb{E}_{Y\sim F^{'}}\left|Y-Y^{'}\right|$


```{r norm_KL_norm_t, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE, dev = tikzDevice::tikz()}

mean(KLBayesnorm_params$sigma2)*out_optim$minimum
mean(KLBayest_params$sigma2)*5/(5-2)

par(mar = c(5.1, 4.5, 4.1, 2.1), mgp = c(3, 1, 0), las = 0)

x <- seq(-5, 15, length.out = 1000)
hist_data1 <- hist(data_norm, breaks = seq(-5, 17, by = 0.2), plot = FALSE)## need to factor in the length of the vector and the number of breaks.
plot(0, 0, type = "n", ylab = "Density", main = expression(paste("KLD")), xlab = "$y$", ylim = c(0, 0.45), xlim = c(-4, 4), cex.lab = 2, cex.axis = 2, cex.main = 2)
#hist_data1$counts <- hist_data1$counts/(n/2)# this guy is made to be equal to 5 as the lengths of each of our bars is 0.2
hist_data1$counts <- hist_data1$counts/(n/5)
plot(hist_data1, add = TRUE, col = "grey")
x <- seq(-10, 15, length.out = 1000)
lines(density(KLBayesnorm_params$y_predict), lwd = 3, col = "red")
legend(3, 0.45, c(expression(paste("N(0, 1)")), "Gaussian"), lty = c(1, 1), lwd = c(3, 3), col = c("grey", "red"), bty = "n", cex = 1.5)
box(which = "plot")

```

```{r norm_beta_norm, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE}
beta <- 0.22 #- Learned using Yonekura & Sugasawa for the betaD

N <- 100000

betaD_w_norm <- weight_calib(data_norm, loss = function(data, theta){beta_loss_norm_sigma2_adj(data, theta[1], theta[2], sigma2_adj = sigma2_adj$minimum, beta)}, theta_0 = c(mean(data_norm), var(data_norm)))


betaBayesnorm_data <- list(n = n, y = matrix(data_norm, nrow = n, ncol = 1), mu_m = mu_0, mu_s = 1/kappa_0, sig_p1 = a_0, sig_p2 = b_0, w = betaD_w_norm, sigma2_adj = sigma2_adj$minimum, beta = beta)
betaBayesnorm <- sampling(object = betaBayesnorm_sigma2_adj_stan, data = betaBayesnorm_data, iter = N+5000, warmup = 5000, chains = 1, cores = 1, control = list(adapt_delta = 0.95))
betaBayesnorm_params <- extract(betaBayesnorm)



```


```{r norm_beta_norm_t, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE, dev = tikzDevice::tikz()}

mean(betaBayesnorm_params$sigma2)*out_optim$minimum
mean(betaBayest_params$sigma2)*5/(5-2)

par(mar = c(5.1, 4.5, 4.1, 2.1), mgp = c(3, 1, 0), las = 0)

x <- seq(-5, 15, length.out = 1000)
hist_data1 <- hist(data_norm, breaks = seq(-5, 17, by = 0.2), plot = FALSE)## need to factor in the length of the vector and the number of breaks.
plot(0, 0, type = "n", ylab = "Density", main = expression(paste("KLD")), xlab = "expression(y)", ylim = c(0, 0.45), xlim = c(-4, 4), cex.lab = 2, cex.axis = 2, cex.main = 2)
#hist_data1$counts <- hist_data1$counts/(n/2)# this guy is made to be equal to 5 as the lengths of each of our bars is 0.2
hist_data1$counts <- hist_data1$counts/(n/5)
plot(hist_data1, add = TRUE, col = "grey")
x <- seq(-10, 15, length.out = 1000)
lines(density(KLBayesnorm_params$y_predict), lwd = 3, col = "red")
lines(density(betaBayesnorm_params$y_predict), lwd = 3, col = "blue")
legend(-4, 0.45, c(expression(paste("N(0, 1)")), "KLD-Bayes", "betaD-Bayes"), lty = c(1, 1, 1), lwd = c(3, 3, 3), col = c("grey", "red", "blue"), bty = "n", cex = 1.5)
box(which = "plot")



```


## Influence Functions {.tabset}


### Loss function gradients

Based on West (1984) these should be the gradient of the likelihood (loss-likelihood)

```{r grad_loss_functions, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE}

grad_KLD_loss_norm <- function(y, mu, sigma2){
  grad_mu <- (y - mu)/sigma2
  grad_sigma2 <- -0.5*1/sigma2 + (y - mu)^2/(2*sigma2^2)
 return(cbind(grad_mu, grad_sigma2))
}

y <- 1.3 
mu <- 0.7
sigma2 <- 1.7

grad_KLD_loss_norm(y, mu, sigma2)
#grad(function(theta){KLD_loss_norm(y, theta[1], theta[2])}, c(mu, sigma2))


grad_KLD_loss_t <- function(y, mu, sigma2, df){
  grad_mu <- (df + 1)/df*(y - mu)/sigma2/(1 + 1/df*(y-mu)^2/sigma2)
  grad_sigma2 <- -0.5*1/sigma2 + (df + 1)/(2*df)*(y - mu)^2/sigma2^2/(1 + 1/df*(y-mu)^2/sigma2)
 return(cbind(grad_mu, grad_sigma2))
}

y <- 1.3 
mu <- 0.7
sigma2 <- 1.7
df <- 5

grad_KLD_loss_t(y, mu, sigma2, df)
#grad(function(theta){KLD_loss_t(y, theta[1], theta[2], df)}, c(mu, sigma2))


grad_beta_loss_norm <- function(y, mu, sigma2, beta){
  
  grad_mu <- -(y - mu)/sigma2*1/(2*pi*sigma2)^(beta/2)*exp(-beta*(y-mu)^2/(2*sigma2))
  grad_sigma2 <- 1/(2*(2*pi)^(beta/2)*(sigma2)^(beta/2 + 1))*exp(-beta*(y-mu)^2/(2*sigma2)) - 
    (y - mu)^2/(2*sigma2^2)*1/(2*pi*sigma2)^(beta/2)*exp(-beta*(y-mu)^2/(2*sigma2)) -
    beta/(2*(2*pi)^(beta/2)*(1+beta)^1.5*((sigma2)^(beta/2 + 1)))

 return(cbind(-grad_mu, -grad_sigma2))#minus cause we actaully want the gradient of the los-likelihood
}

y <- 1.3 
mu <- 0.7
sigma2 <- 1.7
beta <- 0.22 #- Learned using Yonekura & Sugasawa

grad_beta_loss_norm(y, mu, sigma2, beta)
#grad(function(theta){beta_loss_norm(y, theta[1], theta[2], beta)}, c(mu, sigma2))


grad_beta_loss_t <- function(y, mu, sigma2, df, beta){
  
  grad_mu <- -(y - mu)/sigma2*(df + 1)*gamma((df + 1)/2)^beta/(df*gamma(df/2)^beta*(pi*df)^(beta/2)*sigma2^(beta/2))*(1 + 1/df*(y - mu)^2/sigma2)^(-beta*(df + 1)/2 - 1)
  grad_sigma2 <- gamma((df + 1)/2)^beta/(2*gamma(df/2)^beta*(pi*df)^(beta/2)*sigma2^(beta/2 + 1))*(1 + 1/df*(y - mu)^2/sigma2)^(-beta*(df + 1)/2) - 
    (y - mu)^2/sigma2^2*(df + 1)*gamma((df + 1)/2)^beta/(2*df*gamma(df/2)^beta*(pi*df)^(beta/2)*sigma2^(beta/2))*(1 + 1/df*(y - mu)^2/sigma2)^(-beta*(df + 1)/2 - 1) -
    beta*(gamma((df+1)/2)^(beta+1)*gamma((beta*df+beta+df)/2))/(2*(1+beta)*gamma(df/2)^(beta+1)*gamma((beta*df+beta+df+1)/2)*(df)^((beta)/2)*pi^((beta)/2)*sigma2^(beta/2 + 1))

   return(cbind(-grad_mu, -grad_sigma2))  #minus cause we actaully want the gradient of the los-likelihood
}

y <- 1.3 
mu <- 0.7
sigma2 <- 1.7
df <- 5
beta <- 0.22 #- Learned using Yonekura & Sugasawa

grad_beta_loss_t(y, mu, sigma2, df, beta)
#grad(function(theta){beta_loss_t(y, theta[1], theta[2], df, beta)}, c(mu, sigma2))

```


### Gaussian Model - KLD-Bayes - MAP estimates


```{r eps_cont_KL_norm, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE}

KLBayesnorm_data <- list(n = n, y = matrix(data_eps_cont, nrow = n, ncol = 1), mu_m = mu_0, mu_s = 1/kappa_0, sig_p1 = a_0, sig_p2 = b_0, w = 1, sigma2_adj = sigma2_adj$minimum)
KLBayesnorm <- optimizing(object = KLBayesnorm_sigma2_adj_stan, data = KLBayesnorm_data)


KLBayesnorm$par
```

### Student-t Model - KLD-Bayes - MAP estimates

```{r eps_cont_KL_t, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE}

KLBayest_data <- list(n = n, y = matrix(data_eps_cont, nrow = n, ncol = 1), mu_m = mu_0, mu_s = 1/kappa_0, sig_p1 = a_0, sig_p2 = b_0, df = 5, w = 1)
KLBayest <- optimizing(object = KLBayest_stan, data = KLBayest_data)

KLBayest$par
```


### Gaussian Model - betaD-Bayes - MAP estimates

```{r eps_cont_beta_norm, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE}
beta <- 0.22 #- Learned using Yonekura & Sugasawa


betaBayesnorm_data <- list(n = n, y = matrix(data_eps_cont, nrow = n, ncol = 1), mu_m = mu_0, mu_s = 1/kappa_0, sig_p1 = a_0, sig_p2 = b_0, w = betaD_w_norm, sigma2_adj = sigma2_adj$minimum, beta = beta)
betaBayesnorm <- optimizing(object = betaBayesnorm_sigma2_adj_stan, data = betaBayesnorm_data)

betaBayesnorm$par
```

### Student-t Model - betaD-Bayes - MAP estimates

```{r eps_cont_beta_t, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE}
beta <- 0.22 #- Learned using Yonekura & Sugasawa

betaBayest_data <- list(n = n, y = matrix(data_eps_cont, nrow = n, ncol = 1), mu_m = mu_0, mu_s = 1/kappa_0, sig_p1 = a_0, sig_p2 = b_0, df = 5, w = betaD_w_t, beta = beta)
betaBayest <- optimizing(object = betaBayest_stan, data = betaBayest_data)

betaBayest$par
```

### Plotting the Influence Curve

```{r betaD_Influence_Curve_tikz, include = TRUE, echo = TRUE, eval = TRUE, cache = FALSE, fig.height = 3, fig.width = 5, dev = tikzDevice::tikz()}

x_seq <- seq(-10, 10, length.out = 1000)

grad_KLD_norm_eval <- grad_KLD_loss_norm(y = x_seq, mu = KLBayesnorm$par[1], sigma2 = KLBayesnorm$par[2]*sigma2_adj$minimum)

grad_KLD_t_eval <- grad_KLD_loss_t(y = x_seq, mu = KLBayest$par[1], sigma2 = KLBayest$par[2], df= 5)

grad_beta_norm_eval <- grad_beta_loss_norm(y = x_seq, mu = betaBayesnorm$par[1], sigma2 = betaBayesnorm$par[2]*sigma2_adj$minimum, beta)

grad_beta_t_eval <- grad_beta_loss_t(y = x_seq, mu = betaBayest$par[1], sigma2 = betaBayest$par[2], df= 5, beta)

par(mar = c(3.1, 3.3, 1.5, 1.1)) # bottom, left, top, right
#par(mar = c(5.1, 4.1, 4.1, 2.1)) # Default
#par(mgp = c(3, 1, 0)) # Default - location of xlab and ylab, tick-mark labels, tick marks.
par(mgp = c(2.15, 1, 0))
par(cex.lab = 1.25, cex.axis = 1.25, cex.main = 1.25)

plot(x_seq, grad_KLD_norm_eval[,1], xlab = "$y$", ylab = "Influence Function - $\\mu$", type = "l", lwd = 3, col = "red", ylim = c(-2, 2))
points(x_seq, grad_KLD_t_eval[,1], type = "l", lwd = 3, col = "red", lty = 2)
points(x_seq, grad_beta_norm_eval[,1], type = "l", lwd = 3, col = "blue")
points(x_seq, grad_beta_t_eval[,1], type = "l", lwd = 3, col = "blue", lty = 2)
legend("topleft", c("KLD - Gaussian", "KLD - Student's-$t$", "$\\beta$D - Gaussian", "$\\beta$D - Student's-$t$"), lty = c(1, 2, 1, 2), lwd = rep(3, 4), col = c("red", "red", "blue", "blue"), bty = "n", cex = 1.25)
#abline(h = 0)

plot(x_seq, grad_KLD_norm_eval[,2], xlab = "$y$", ylab = "Influence Function - $\\sigma^2$", type = "l", lwd = 3, col = "red", ylim = c(-1, 4))
points(x_seq, grad_KLD_t_eval[,2], type = "l", lwd = 3, col = "red", lty = 2)
points(x_seq, grad_beta_norm_eval[,2], type = "l", lwd = 3, col = "blue")
points(x_seq, grad_beta_t_eval[,2], type = "l", lwd = 3, col = "blue", lty = 2)
#abline(h = 0)
#legend("top", c("KLD - Gaussian", "KLD - Student's-$t$", "$\\beta$D - Gaussian", "$\\beta$D - Student's-$t$"), lty = c(1, 2, 1, 2), lwd = rep(3, 4), col = c("red", "red", "blue", "blue"), bty = "n", cex = 1.25)
box(which = "plot")

```









