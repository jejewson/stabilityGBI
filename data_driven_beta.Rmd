---
title: "data_driven_beta"
author: "Jack Jewson"
date: "Jan 2024"
output: html_document
---

Implementing Yonekura & Sugasawa's method to select $\beta$ for the Gaussian vs Student's-$t$ experiments


## Preliminaries {.tabset}

### Working directory

Setting the Working Directory as the folder where these files are stored

```{r wkd, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE}

my.dir <- "/home/usuario/Documents/Barcelona_Yr1/StabilityGeneralBayes"

my.dir.data <- "/home/usuario/Documents/Barcelona_Yr1/datasets/HyvarinenRegressions"
```

### Packages

Loading required packages

```{r pcks, include = TRUE, echo = TRUE, eval = TRUE, cache = FALSE}

library(rstan)
rstan_options(auto_write = TRUE)

library(metRology)

library(numDeriv)
library(RColorBrewer)

```

### stan files

Compiling the required stan files

```{r stan, include = TRUE, echo = TRUE, eval = TRUE, cache = FALSE}

setwd(my.dir)

#betaBayesnorm_stan <- stan_model(file = "betaBayesnorm_var.stan")
betaBayesnorm_sigma2_adj_stan <- stan_model(file = "betaBayesnorm_var_sigma2_adj.stan")

betaBayesnorm_linearmodel_sigma2_adj_stan <- stan_model(file = "betaBayesnorm_linearmodel_sigma2_adj.stan")

#betaBayest_stan <- stan_model(file = "betaBayest_var.stan")


```


### functions

#### betaD H-score functions

```{r gradient_functions, include=TRUE, echo=TRUE, eval=TRUE, cache=TRUE}

## These are not strictly speaking losses are they? They are negative losses.
log_score_norm<- function(x, mu, sigma2, w){
 return(w*dnorm(x, mu, sqrt(sigma2), log=TRUE))
}

grad_log_score_norm <- function(x, mu, sigma2, w){
 return(-w*(x-mu)/sigma2)
}

Laplacian_log_score_norm <- function(x, mu, sigma2, w){
 return(-w*1/sigma2)
}

betaD_score_norm <- function(x, mu, sigma2, beta, w){
  if(beta == 0){
    return(w*dnorm(x, mu, sqrt(sigma2), log = TRUE))
  } else{
    integral_term<-1/((2*pi)^(beta/2)*(1+beta)^1.5*(sigma2^(beta/2)))
    likelihood_term<- (1/beta)*dnorm(x, mu, sqrt(sigma2))^(beta)
    return(w*likelihood_term - integral_term)
  }
}

grad_betaD_score_norm <- function(x, mu, sigma2, beta, w){
 return(-w*(x-mu)/((2*pi)^(beta/2)*sigma2^((beta+2)/2))*exp(-beta*(x-mu)^2/(2*sigma2)))
}

Laplacian_betaD_score_norm <- function(x, mu, sigma2, beta, w){
  return(w/((2*pi)^(beta/2)*sigma2^((beta+2)/2))*exp(-beta*(x-mu)^2/(2*sigma2))*(beta*(x-mu)^2/(sigma2)-1))
}


```

#### Yonekura & Sugasawa

Functions to evaluate values of $\beta$ according to the leave-one-out Hyvarinen-score of Yonekura & Sugasawa

```{r H-score_betaD, include=TRUE, echo=TRUE, eval=TRUE, cache=TRUE}

H_score_betaD_norm_YS <- function(data, beta, w){
  #a_0 <- 2
  #b_0 <- 5
  #mu_0 <- 0
  #kappa_0 <- 1/5
  mu_0 <- 0
  kappa_0 <- 1/10
  a_0 <- 0.01
  b_0 <- 0.01
  v_0 <- 1/kappa_0
  n <- length(data)
 
  ## Sampling from the full posterior 
  N_MCMC <- 1000
  betaBayesnorm_data <- list(n = n, y = matrix(data, nrow = n, ncol = 1), mu_m = mu_0, mu_s = 1/kappa_0, sig_p1 = a_0, sig_p2 = b_0, w = w, sigma2_adj = sigma2_adj$minimum, beta = beta)
  betaBayesnorm <- sampling(object = betaBayesnorm_sigma2_adj_stan, data = betaBayesnorm_data, iter = 1000 + N_MCMC, chains = 1, cores = 1
  #, control = list(adapt_delta = 0.9, stepsize = 0.01, max_treedepth = 20)
  , control = list(adapt_delta = 0.95)
  , refresh = 0
  )
  betaBayesnorm_params <- extract(betaBayesnorm)
  
  posterior_summaries <- matrix(NA, nrow = 1, ncol = 2) 
  sigma2_samp <- betaBayesnorm_params$sigma2
  mu_samp <- betaBayesnorm_params$mu
  posterior_summaries[1, ] <- c(mean(mu_samp), mean(sigma2_samp))
  
  ## Estimating the H-score
  H_score <- rep(NA, n)
  for(i in 1:n){
  ## Estimating the H-score
    H_score[i] <- 2* mean(Laplacian_betaD_score_norm(data[i], mu_samp, sigma2_samp*sigma2_adj$minimum, beta, w) + (grad_betaD_score_norm(data[i], mu_samp, sigma2_samp*sigma2_adj$minimum, beta, w))^2) - (mean(grad_betaD_score_norm(data[i], mu_samp, sigma2_samp*sigma2_adj$minimum, beta, w)))^2
    if(i%%(n/10) == 1){
      #cat("Time Point", i, "done", "\n")
    }
  }
  return(list("H_score" = H_score, "posterior_summaries" = posterior_summaries))
}

H_score_betaD_regression_YS <- function(data_y, data_X, beta, w){
  mu_0 <- 0
  v_0 <- 5
  a_0 <- 2
  b_0 <- 0.5
  n <- length(data_y)
  p <- ncol(data_X)
 
  ## Sampling from the full posterior 
  N_MCMC <- 1000
  betaBayesnorm_linearmodel_data <- list(n = n, p = p, y = as.matrix(data_y, nrow = n, ncol = 1), X = data_X, mu_beta = mu_0, beta_s = v_0, sig_p1 = a_0, sig_p2 = b_0, w = w, beta_p = beta, sigma2_adj = sigma2_adj$minimum)
  betaBayesnorm_linearmodel <- sampling(object =  betaBayesnorm_linearmodel_sigma2_adj_stan, data = betaBayesnorm_linearmodel_data, iter = 1000 + N_MCMC, chains = 1, cores = 1
  #, control = list(adapt_delta = 0.9, stepsize = 0.01, max_treedepth = 20)
  , control = list(adapt_delta = 0.95)
  , refresh = 0
  )
  betaBayesnorm_linearmodel_params <- extract(betaBayesnorm_linearmodel)
  
  posterior_summaries <- matrix(NA, nrow = 1, ncol = p + 1) 
  sigma2_samp <- betaBayesnorm_linearmodel_params$sigma2*sigma2_adj$minimum
  beta_samp <- betaBayesnorm_linearmodel_params$beta
  posterior_summaries[1, ] <- c(colMeans(beta_samp), mean(sigma2_samp))
  
  ## Estimating the H-score
  H_score <- rep(NA, n)
  for(i in 1:n){
  ## Estimating the H-score
    H_score[i] <- 2* mean(Laplacian_betaD_score_norm(data_y[i], drop(data_X[i,]%*%t(beta_samp)), sigma2_samp, beta, w) + (grad_betaD_score_norm(data_y[i], drop(data_X[i,]%*%t(beta_samp)), sigma2_samp, beta, w))^2) - (mean(grad_betaD_score_norm(data_y[i], drop(data_X[i,]%*%t(beta_samp)), sigma2_samp, beta, w)))^2
    if(i%%(n/10) == 1){
      #cat("Time Point", i, "done", "\n")
    }
  }
  return(list("H_score" = H_score, "posterior_summaries" = posterior_summaries))
}


```

## Gaussian and Student-t TVD Neighbourhood 

Constructing the TVD neighbourhood including the Gaussian and the Student's-t likelihood

```{r t_normal_neighbourhood, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE, dev = tikzDevice::tikz()}


## estimating sigma^2_adj to match the quartiles
sigma2_adj_fun <- function(sigma2_adj){
 return(abs(qt.scaled(0.25, df = 5, 0, 1)-qnorm(0.25, 0, sqrt(sigma2_adj*1))))
}

sigma2_adj <- optimize(sigma2_adj_fun, 1, lower = 0, upper = 1000)
sigma2_adj$minimum

## Above tells us this is all valid for any of these parameters!
df <- 5
mu <- 0
sigma2 <- 1

par(mar = c(5.1, 4.5, 4.1, 2.1), mgp = c(3, 1, 0), las = 0)
x <- seq(-5, 5, length.out = 1000)
plot(x, dnorm(x, mu, sqrt(sigma2*sigma2_adj$minimum)), type = "l", lwd = 3, col = "red", xlab = "$y$", ylab = "Density", cex.lab = 2, cex.axis = 2)
points(x, dt.scaled(x, df, mu, sqrt(sigma2)), type = "l", lwd = 3, col = "blue")
legend(-5, 0.35, c("Gaussian", "Student's-t"), lty = c(1, 1), lwd = c(3, 3), col = c("red", "blue"), bty = "n", cex = 1.2)
box(which = "plot")

par(mar = c(5.1, 4.5, 4.1, 2.1), mgp = c(3, 1, 0), las = 0)
plot(x, pnorm(x, mu, sqrt(sigma2*sigma2_adj$minimum)), type = "l", lwd = 3, col = "red", xlab = "$y$", ylab = "Cumulative Density", cex.lab = 2, cex.axis = 2)
points(x, pt.scaled(x, df, mu, sqrt(sigma2)), type = "l", lwd = 3, col = "blue")


```

## Epsilon Contaminated Data

```{r eps_cont_data_sim, include = TRUE, echo = TRUE, eval = TRUE, cache = TRUE}
n <- 1000
set.seed(3)
mu_c <- 5
sig_c <- 3
mu <- 0
sig <- 1
eps <- 0.1

cont <- sample(c(0, 1), n, replace = TRUE, prob = c(1-eps, eps))
data_eps_cont <- (1-cont)*rnorm(n, mu, sig)+cont*rnorm(n, mu_c, sig_c)

cont_ind <- which(cont == 1)


```

### Yonekura & Sugasawa: Selecting beta


```{r selecting_beta_true, include=TRUE, echo=TRUE, eval=TRUE, cache=TRUE, results='hide'}

#beta_vect <- c(0.001, 0.01, seq(0.05, 1, by=0.05))
beta_vect <- seq(0.1, 0.3, by=0.01)

q <- length(beta_vect)

H_score_norm_eps_cont_beta_select <- rep(NA, q)
H_score_norm_eps_cont_beta_select_params <- list()

for(i in 1:q){
  temp <- H_score_betaD_norm_YS(data_eps_cont, beta = beta_vect[i], w = 1)
  
  H_score_norm_eps_cont_beta_select[i] <- sum(temp$H_score)
  H_score_norm_eps_cont_beta_select_params[[i]] <- temp$posterior_summaries[1, ]

  cat("Parameter", beta_vect[i], "done", "posterior-H-score =", H_score_norm_eps_cont_beta_select[i], "\n")
}


```

```{r selecting_beta_true_diag, include=TRUE, echo=TRUE, eval=TRUE, cache=FALSE}

par(mar=c(5.1, 4.5, 4.1, 2.1), mgp=c(3, 1, 0), las=0)
plot(beta_vect+1, H_score_norm_eps_cont_beta_select, type="b", lwd=3, xlab=expression(beta), ylab="H-score", main = "eps_cont", cex.lab=2, cex.axis=2, cex.main=2 )

min_ind_eps_cont <- which.min(H_score_norm_eps_cont_beta_select)
beta_vect[min_ind_eps_cont]

H_score_norm_eps_cont_beta_select_params[[min_ind_eps_cont]]

H_score_norm_eps_cont_beta_select_params[[1]]

library(metRology)
par(mar=c(5.1, 4.5, 4.1, 2.1), mgp=c(3, 1, 0), las=0)
x <- seq(-5, 15, length.out = 1000)
hist_data1 <- hist(data_eps_cont[-cont_ind], breaks = seq(-5, 17, by = 0.2), plot = FALSE)## need to factor in the length of the vector and the number of breaks.
hist_data2 <- hist(data_eps_cont[cont_ind], breaks = seq(-5, 17, by = 0.2), plot = FALSE)
plot(0, 0, type = "n", ylab = "Density", main = expression(paste(beta, "D")), xlab = "$y$", ylim = c(0, 0.45), xlim = c(-5, 12), cex.lab = 2, cex.axis = 2, cex.main = 2)
#hist_data1$counts <- hist_data1$counts/(n/2)# this guy is made to be equal to 5 as the lengths of each of our bars is 0.2
hist_data1$counts <- hist_data1$counts/(n/5)
hist_data2$counts <- hist_data2$counts/(n/5)
plot(hist_data1, add = TRUE, col = "grey")
plot(hist_data2, add = TRUE, col = "black")
points(x, dnorm(x, H_score_norm_eps_cont_beta_select_params[[min_ind_eps_cont]][1], sd = sqrt(H_score_norm_eps_cont_beta_select_params[[min_ind_eps_cont]][2]*sigma2_adj$minimum)), type="l", lwd=4, col="blue")
points(x, dnorm(x, 0, 1), type="l", lwd=4, lty=2)
legend(3, 0.45, c("N(0, 1) observations", paste("beta = ", beta_vect[min_ind_eps_cont]), "N(0, 1) density"), lty=c(1, 1, 2), lwd=rep(4, 3), col=c("grey", "blue", "black"), bty="n", cex=1.5)
box(which = "plot")


```

Estimated 0.2 from the first run 
Estimated 0.22 after zooming in


## DLD data {.tabset}

### Data Loading

Loading the data and defining the response and set of possible predictors.

```{r DLD_dataload, include=TRUE,echo=TRUE, eval=TRUE,cache=FALSE}
#setwd(paste0(my.dir.data, "/data"))

dld_data <- read.table(paste0(my.dir.data, "/dld.txt"), header = TRUE, sep = '\t')

dld_y <- as.vector(scale(dld_data[,1]))
dld_X <- cbind(scale(as.matrix(dld_data[,c(-1, -56, -57, -58)])), as.matrix(dld_data[,c(56, 57, 58)]))

n_obs_dld <- length(dld_y)
n_obs_dld
p_dim_dld <- ncol(dld_X)

```

### Data Preprocessing: PCA variable selection

A principal components analysis of the set of predictors for DLD. Selecting the 5 variables with the highest loading in each of the first three principal components.

```{r DLD_PCA, include=TRUE,echo=TRUE, eval=TRUE,cache=FALSE}

PCA_dld_X <- princomp(dld_X)

PCA_selected_names <- unique(c(names(sort(abs(PCA_dld_X$loadings[,1]), decreasing = TRUE)[1:5]),
names(sort(abs(PCA_dld_X$loadings[,2]), decreasing = TRUE)[1:5])
, names(sort(abs(PCA_dld_X$loadings[,3]), decreasing = TRUE)[1:5])
))

index_PCA_selected <- rep(NA, 15)
for(i in 1:15){
  index_PCA_selected[i] <- which(colnames(dld_X) == PCA_selected_names[i])
}

index_PCA_selected <- sort(index_PCA_selected)

index_PCA_selected

dld_X_sparse <- cbind(rep(1, n_obs_dld), dld_X[,index_PCA_selected])

p_dim_sparse_dld <- ncol(dld_X_sparse)

p_dim_sparse_dld

```

Calculating the Gaussian MLE value to provide inital values for the MAP optimisation. Further, re-scaling the $y$'s in order to avoid instabilities from their conditional variance being so small. 

```{r dld_PCA_var_adjust, include=TRUE,echo=TRUE, eval=TRUE,cache=FALSE}

MLE_dld_sparse <- lm(dld_y ~ dld_X_sparse + 0)

(summary(MLE_dld_sparse)$sigma)**2

dld_y_scaled <- dld_y / (summary(MLE_dld_sparse)$sigma)

MLE_dld_sparse_scaled  <- lm(dld_y_scaled  ~ dld_X_sparse + 0)


```

### Yonekura & Sugasawa: Selecting beta

```{r selecting_beta_dld, include=TRUE, echo=TRUE, eval=TRUE, cache=TRUE, results='hide'}

#beta_vect <- c(0.001, 0.01, seq(0.05, 1, by=0.05))
beta_vect <- seq(0.25, 0.45, by=0.01)

q <- length(beta_vect)

H_score_norm_dld_beta_select <- rep(NA, q)
H_score_norm_dld_beta_select_params <- list()

for(i in 1:q){
  temp <- H_score_betaD_regression_YS(data_y = dld_y_scaled, data_X = dld_X_sparse, beta = beta_vect[i], w = 1)
  
  H_score_norm_dld_beta_select[i] <- sum(temp$H_score)
  H_score_norm_dld_beta_select_params[[i]] <- temp$posterior_summaries[1, ]

  cat("Parameter", beta_vect[i], "done", "posterior-H-score =", H_score_norm_dld_beta_select[i], "\n")
}


```

```{r selecting_beta_dld_diag, include=TRUE, echo=TRUE, eval=TRUE, cache=FALSE}

par(mar=c(5.1, 4.5, 4.1, 2.1), mgp=c(3, 1, 0), las=0)
plot(beta_vect+1, H_score_norm_dld_beta_select, type="b", lwd=3, xlab=expression(beta), ylab="H-score", main = "dld", cex.lab=2, cex.axis=2, cex.main=2 )

min_ind_dld <- which.min(H_score_norm_dld_beta_select)
beta_vect[min_ind_dld]

H_score_norm_dld_beta_select_params[[min_ind_dld]]
MLE_dld_sparse_scaled

x_seq <- seq(-20, 20, length.out = 2000)

hist((dld_y_scaled - dld_X_sparse%*%H_score_norm_dld_beta_select_params[[min_ind_dld]][1:p_dim_sparse_dld])/ sqrt(H_score_norm_dld_beta_select_params[[min_ind_dld]][p_dim_sparse_dld + 1])
     , breaks = 50, probability = TRUE, main = "betaD - Gaussian", xlab = "$(y - X\\hat{\\theta})/\\hat{\\sigma}$", ylim = c(0, 0.6), xlim = c(-10, 10))
points(x_seq, dnorm(x_seq, 0, 1), lwd = 3, type = "l", lty = 1, col = "red")
box()

hist((dld_y_scaled - dld_X_sparse%*%H_score_norm_dld_beta_select_params[[1]][1:p_dim_sparse_dld])/ sqrt(H_score_norm_dld_beta_select_params[[1]][p_dim_sparse_dld + 1])
     , breaks = 50, probability = TRUE, main = "betaD - Gaussian", xlab = "$(y - X\\hat{\\theta})/\\hat{\\sigma}$", ylim = c(0, 0.6), xlim = c(-10, 10))
points(x_seq, dnorm(x_seq, 0, 1), lwd = 3, type = "l", lty = 1, col = "red")
box()


```

Estimated 0.35 form the first run 
Estimates 0.34 from zooming in

## TGFB172 data {.tabset}

### Data Loading

Loading the data and defining the response and set of possible predictors.

```{r TGFB172_dataload, include=TRUE,echo=TRUE, eval=TRUE, cache=TRUE}
#setwd(paste(my.dir, "/data", sep = ""))

tgfb10000_data <- read.table(paste0(my.dir.data, "/tgfb_10000.txt"), header = TRUE, sep='\t')


tgfb10000_y <- as.vector(tgfb10000_data[,1])
tgfb10000_X <- cbind(1,as.matrix(tgfb10000_data[,-1]))

n_obs_tgfb10000 <- length(tgfb10000_y)
p_dim_tgfb10000 <- ncol(tgfb10000_X)

n_obs_tgfb10000
```

### Data preprocessing

Identifying the genes available in the set of predictors that appear in the ‘TGF-B1 pathway’.

#### Gene symbols and KEGG gene ids from the dataset

Identifying the gene symbols corresponding to each variable in the data and transforming these into the KEGG number.

```{r TGFB172_gene_symbols, include=TRUE, echo=TRUE, eval=TRUE, cache=TRUE}

#gene_symbol10000 <- rep(NA, p_dim_tgfb10000 - 1) ## -1 because of the intercept 
#for(i in 1:(p_dim_tgfb10000 - 1)){
#  gene_symbol10000[i] <- mget(sub("X", "", colnames(tgfb10000_data)[1 + i]), hgu133plus2SYMBOL)[[1]] #gene symbol
#}

#NA_IDs <- which(is.na(gene_symbol10000) == TRUE)
#sym_data10000 = gene_symbol10000[-NA_IDs]    
#EG_IDs_data = mget(sym_data10000, revmap(org.Hs.egSYMBOL), ifnotfound = NA)

#EG_ICs_data <- unlist(EG_IDs_data)

```

#### KEGG gene pathways

Identifying the genes that appear in the ‘TGF-B1 pathway’.

```{r KEGG_gene_pathways, include=TRUE, echo=TRUE, eval=TRUE, cache=TRUE}

#TGFB_query <- keggGet(c("hsa:7040")) # https://www.genome.jp/dbget-bin/www_bget?hsa:7040+hsa:7042+hsa:7043

#TGFB_pathway_KEGGNumbers <- rep(NA, length(TGFB_query[[1]]$PATHWAY))
#for(i in 1:length(TGFB_query[[1]]$PATHWAY)){
#  TGFB_pathway_KEGGNumbers[i] <- sub("hsa0", "", names(TGFB_query[[1]]$PATHWAY)[i])
#}

```

#### Gene symbol to KEGG gene ids

Intersecting the ‘TGF-B1 pathway’ with the available set of predictors.

```{r Gene_symbol_to_KEGG_gene_pathways, include=TRUE, echo=TRUE, eval=TRUE, cache=TRUE}

#pathway_data <- intersect(TGFB_pathway_KEGGNumbers, EG_ICs_data)

#ID10000_pathway <- rep(NA, length(pathway_data))
#for(i in 1:length(pathway_data)){
#  ID10000_pathway[i] <- which(EG_ICs_data == pathway_data[i])
#}

#ID10000_pathway <- sort(ID10000_pathway)

#ID10000_pathway <- c(1553, 2368, 3701, 6813, 7222, 8838, 9013)
## I think the above packages conflict with stan so I just hard code
## I took this from selecting_loss_jointHyvarinen_LaplaceApprox_TGFB.html

#tgfb10000_X_pathway <- tgfb10000_X[, - c(1 + NA_IDs)][,c(1, 1 + ID10000_pathway)]

## so we have the intercept and
ID10000_pathway_colnames <- c("X227899_at", "X211302_s_at", "X214594_x_at", "X213996_at", "X215239_x_at", "X202659_at", "X207009_at")
ID10000_pathway <- rep(NA, length(ID10000_pathway_colnames))
for(j in 1:length(ID10000_pathway_colnames)){
  ID10000_pathway[j] <- which(colnames(tgfb10000_X) == ID10000_pathway_colnames[j])
}
ID10000_pathway

tgfb10000_X_pathway <- tgfb10000_X[,c(1, ID10000_pathway)]


p_dim_sparse_tgfb1000 <- ncol(tgfb10000_X_pathway)

p_dim_sparse_tgfb1000

#detach("package:hgu133plus2.db",  unload=TRUE)
#detach("package:KEGGREST",  unload=TRUE)
#detach("package:org.Hs.eg.db",  unload=TRUE)
```

#### MLE values

Calculating the Gaussian MLE value to provide initial values for the MAP optimisation.

```{r tgfb10000_pathway_indicies_MLE, include=TRUE, echo=TRUE, eval=TRUE, cache=TRUE}

MLE_tgfb10000_pathway <- lm(tgfb10000_y ~ tgfb10000_X_pathway + 0)
MLE_tgfb10000_pathway$coefficients
```

### Yonekura & Sugasawa: Selecting beta

```{r selecting_beta_tgfb, include=TRUE, echo=TRUE, eval=TRUE, cache=TRUE, results='hide'}

#beta_vect <- c(0.001, 0.01, seq(0.05, 1, by=0.05))
beta_vect <- c(0.001, seq(0.01, 0.15, by=0.01))

q <- length(beta_vect)

H_score_norm_tgfb_beta_select <- rep(NA, q)
H_score_norm_tgfb_beta_select_params <- list()

for(i in 1:q){
  temp <- H_score_betaD_regression_YS(data_y = tgfb10000_y, data_X = tgfb10000_X_pathway, beta = beta_vect[i], w = 1)
  
  H_score_norm_tgfb_beta_select[i] <- sum(temp$H_score)
  H_score_norm_tgfb_beta_select_params[[i]] <- temp$posterior_summaries[1, ]

  cat("Parameter", beta_vect[i], "done", "posterior-H-score =", H_score_norm_tgfb_beta_select[i], "\n")
}


```

```{r selecting_beta_tgfb_diag, include=TRUE, echo=TRUE, eval=TRUE, cache=FALSE}

par(mar=c(5.1, 4.5, 4.1, 2.1), mgp=c(3, 1, 0), las=0)
plot(beta_vect+1, H_score_norm_tgfb_beta_select, type="b", lwd=3, xlab=expression(beta), ylab="H-score", main = "tgfb", cex.lab=2, cex.axis=2, cex.main=2 )

min_ind_tgfb <- which.min(H_score_norm_tgfb_beta_select)
beta_vect[min_ind_tgfb]

H_score_norm_tgfb_beta_select_params[[min_ind_tgfb]]
MLE_tgfb10000_pathway$coefficients

x_seq <- seq(-20, 20, length.out = 2000)

hist((tgfb10000_y - tgfb10000_X_pathway%*%H_score_norm_tgfb_beta_select_params[[min_ind_tgfb]][1:p_dim_sparse_tgfb1000])/ sqrt(H_score_norm_tgfb_beta_select_params[[min_ind_tgfb]][p_dim_sparse_tgfb1000 + 1])
     , breaks = 50, probability = TRUE, main = "betaD - Gaussian", xlab = "$(y - X\\hat{\\theta})/\\hat{\\sigma}$", ylim = c(0, 0.6), xlim = c(-10, 10))
points(x_seq, dnorm(x_seq, 0, 1), lwd = 3, type = "l", lty = 1, col = "red")
box()

hist((tgfb10000_y - tgfb10000_X_pathway%*%H_score_norm_tgfb_beta_select_params[[1]][1:p_dim_sparse_tgfb1000])/ sqrt(H_score_norm_tgfb_beta_select_params[[1]][p_dim_sparse_tgfb1000 + 1])
     , breaks = 50, probability = TRUE, main = "betaD - Gaussian", xlab = "$(y - X\\hat{\\theta})/\\hat{\\sigma}$", ylim = c(0, 0.6), xlim = c(-10, 10))
points(x_seq, dnorm(x_seq, 0, 1), lwd = 3, type = "l", lty = 1, col = "red")
box()


```

Estimated 0.05 form the first run 
Estimates 0.03 from zooming in

